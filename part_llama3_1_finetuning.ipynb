{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "cellView": "form",
    "id": "7d9bbf86da5e"
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99c1c3fc2ca5"
   },
   "source": [
    "# Vertex AI Model Garden - Llama 3.1 Finetuning\n",
    "\n",
    "<table><tbody><tr>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Fmodel_garden%2Fmodel_garden_pytorch_llama3_1_finetuning.ipynb\">\n",
    "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_llama3_1_finetuning.ipynb\">\n",
    "      <img alt=\"GitHub logo\" src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" width=\"32px\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</tr></tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3de7470326a2"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates finetuning and deploying Llama 3.1 models with Vertex AI. All of the examples in this notebook use parameter efficient finetuning methods [PEFT (LoRA)](https://github.com/huggingface/peft) to reduce training and storage costs. LoRA (Low-Rank Adaptation) is one approach of Parameter Efficient FineTuning (PEFT), where pretrained model weights are frozen and rank decomposition matrices representing the change in model weights are trained during finetuning.\n",
    "After finetuning, we can deploy models on Vertex with GPU.\n",
    "\n",
    "\n",
    "### Objective\n",
    "\n",
    "- Finetune Llama 3.1 models with Vertex AI Custom Training Jobs.\n",
    "- Deploy finetuned Llama 3.1 models on Vertex AI Prediction.\n",
    "- Send prediction requests to your finetuned Llama 3.1 models.\n",
    "\n",
    "\n",
    "### Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "264c07757582"
   },
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing google-cloud-aiplatform\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ryptography (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ryptography (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Installing tensorflow\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ryptography (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ryptography (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Installing dependencies complete\n"
     ]
    }
   ],
   "source": [
    "print(\"Installing google-cloud-aiplatform\")\n",
    "! pip3 install -q google-cloud-aiplatform\n",
    "print(\"Installing tensorflow\")\n",
    "!pip3 install -q tensorflow\n",
    "\n",
    "print(\"Installing dependencies complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Google Cloud project\n",
    "\n",
    "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "2. [Make sure that you have GPU quota for Vertex Training (finetuing) and Vertex Prediction (serving)](https://cloud.google.com/docs/quotas/view-manage). The quota name for Vertex Training is \"Custom model training your-gpu-type per region\" and the quota name for Vertex Prediction is \"Custom model serving your-gpu-type per region\" such as `Custom model training Nvidia L4 GPUs per region` and `Custom model serving Nvidia L4 GPUs per region` for L4 GPUs. [Submit a quota increase request](https://cloud.google.com/docs/quotas/view-manage#requesting_higher_quota) if additional quota is needed. At minimum, running this notebook requires 4 L4s for finetuning and 1 L4 for serving. More GPUs may be needed for larger models and different finetuning configurations. To secure GPUs for larger models, ask your customer engineer to get you allowlisted for a Shared Reservation or a Dynamic Workload Scheduler.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'vertex-ai-samples' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/GoogleCloudPlatform/vertex-ai-samples.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define environment variables\n",
    "\n",
    "Set the following variables for the experiment environment. The specified Cloud Storage bucket (`BUCKET_URI`) should be located in the specified region (`REGION`). Note that a multi-region bucket (eg. \"us\") is not considered a match for a single region covered by the multi-region range (eg. \"us-central1\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "cellView": "form",
    "id": "855d6b96f291",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "region is not ASIA Southeast\n",
      "Enabling Vertex AI API and Compute Engine API.\n",
      "Operation \"operations/acat.p2-255766800726-d8e445bf-34f7-4336-9809-18ebbeb61708\" finished successfully.\n",
      "Using this GCS Bucket: gs://my-project-0004-346516-llama31-asia-southeast1\n",
      "Updated property [core/project].\n",
      "Creating gs://my-project-0004-346516-llama31-asia-southeast1/...\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import os\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from typing import Tuple\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "import tensorflow\n",
    "\n",
    "import socket\n",
    "import re\n",
    "\n",
    "common_util = importlib.import_module(\n",
    "    \"vertex-ai-samples.community-content.vertex_model_garden.model_oss.notebook_util.common_util\"\n",
    ")\n",
    "models, endpoints = {}, {}\n",
    "\n",
    "UNIQUE_PREFIX = \"llama31\" #socket.gethostname()\n",
    "UNIQUE_PREFIX = re.sub('[^A-Za-z0-9]+', '', UNIQUE_PREFIX)\n",
    "\n",
    "# Cloud project id.\n",
    "PROJECT_IDS = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_IDS[0]  # @param {type:\"string\"}\n",
    "\n",
    "# The region you want to launch jobs in.\n",
    "PREFIX_NUM_ONLY=int(str(re.search(r'\\d+', UNIQUE_PREFIX).group()))\n",
    "REGION_ALLOCATE=PREFIX_NUM_ONLY%3\n",
    "if REGION_ALLOCATE == 0:\n",
    "    REGION = \"asia-southeast1\"\n",
    "elif REGION_ALLOCATE == 1:\n",
    "    REGION = \"us-central1\"\n",
    "    print(\"region is not ASIA Southeast\")\n",
    "else:\n",
    "    REGION = \"europe-west4\"\n",
    "\n",
    "REGION = \"asia-southeast1\"\n",
    "# Enable the Vertex AI API and Compute Engine API, if not already.\n",
    "print(\"Enabling Vertex AI API and Compute Engine API.\")\n",
    "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com\n",
    "\n",
    "# The service account looks like:\n",
    "# '@.iam.gserviceaccount.com'\n",
    "# Please go to https://cloud.google.com/iam/docs/service-accounts-create#iam-service-accounts-create-console\n",
    "# and create service account with `Vertex AI User` and `Storage Object Admin` roles.\n",
    "# The service account for deploying fine tuned model.\n",
    "SERVICE_ACCOUNT = !(gcloud config get-value core/account)  # @param {type:\"string\"}\n",
    "SERVICE_ACCOUNT = SERVICE_ACCOUNT[0]  # @param {type:\"string\"}\n",
    "\n",
    "# The Cloud Storage bucket for storing experiments output.\n",
    "# Remove prefix gs://, e.g. foo_bucket.\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-{UNIQUE_PREFIX}-{REGION}\"\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\"  # @param {type:\"string\"}\n",
    "print(f\"Using this GCS Bucket: {BUCKET_URI}\")\n",
    "\n",
    "! gcloud config set project $PROJECT_ID\n",
    "! gcloud storage buckets create {BUCKET_URI} --project={PROJECT_ID} --location={REGION}\n",
    "\n",
    "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
    "MODEL_BUCKET = os.path.join(BUCKET_URI, \"llama3_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Vertex AI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Vertex AI API.\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing Vertex AI API.\")\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The pre-built serving docker image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20240721_0916_RC00\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access Llama 3.1 models on Vertex AI for GPU based serving\n",
    "The original models from Meta are converted into the Hugging Face format for serving in Vertex AI.\n",
    "Accept the model agreement to access the models:\n",
    "1. Open the [Llama 3.1 model card](https://console.cloud.google.com/vertex-ai/publishers/meta/model-garden/llama3_1) from [Vertex AI Model Garden](https://cloud.google.com/model-garden).\n",
    "2. Review and accept the agreement in the pop-up window on the model card page. If you have previously accepted the model agreement, there will not be a pop-up window on the model card page and this step is not needed.\n",
    "3. After accepting the agreement of Llama 3.1, a `gs://` URI containing Llama 3.1 pretrained and finetuned models will be shared.\n",
    "4. Paste the URI in the `VERTEX_AI_MODEL_GARDEN_LLAMA3_1` field below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://vertex-model-garden-public-us/llama3.1/Meta-Llama-3.1-8B/README.md [Content-Type=text/markdown]...\n",
      "Copying gs://vertex-model-garden-public-us/llama3.1/Meta-Llama-3.1-8B/config.json [Content-Type=application/json]...\n",
      "Copying gs://vertex-model-garden-public-us/llama3.1/Meta-Llama-3.1-8B/generation_config.json [Content-Type=application/json]...\n",
      "Copying gs://vertex-model-garden-public-us/llama3.1/Meta-Llama-3.1-8B/model-00001-of-00004.safetensors [Content-Type=application/octet-stream]...\n",
      "Copying gs://vertex-model-garden-public-us/llama3.1/Meta-Llama-3.1-8B/model-00002-of-00004.safetensors [Content-Type=application/octet-stream]...\n",
      "Copying gs://vertex-model-garden-public-us/llama3.1/Meta-Llama-3.1-8B/model-00004-of-00004.safetensors [Content-Type=application/octet-stream]...\n",
      "Copying gs://vertex-model-garden-public-us/llama3.1/Meta-Llama-3.1-8B/model-00003-of-00004.safetensors [Content-Type=application/octet-stream]...\n",
      "Copying gs://vertex-model-garden-public-us/llama3.1/Meta-Llama-3.1-8B/special_tokens_map.json [Content-Type=application/json]...\n",
      "Copying gs://vertex-model-garden-public-us/llama3.1/Meta-Llama-3.1-8B/tokenizer.json [Content-Type=application/json]...\n",
      "Copying gs://vertex-model-garden-public-us/llama3.1/Meta-Llama-3.1-8B/tokenizer_config.json [Content-Type=application/json]...\n",
      "Copying gs://vertex-model-garden-public-us/llama3.1/Meta-Llama-3.1-8B/model.safetensors.index.json [Content-Type=application/json]...\n",
      "- [11/11 files][ 15.0 GiB/ 15.0 GiB] 100% Done     0.0 B/s                      \n",
      "Operation completed over 11 objects/15.0 GiB.                                    \n"
     ]
    }
   ],
   "source": [
    "# The Llama 3.1 base model.\n",
    "base_model_name = \"Meta-Llama-3.1-8B\"  # @param [\"meta-llama/Meta-Llama-3.1-8B\", \"meta-llama/Meta-Llama-3.1-8B-Instruct\", \"meta-llama/Meta-Llama-3.1-70B\", \"meta-llama/Meta-Llama-3.1-70B-Instruct\"] {isTemplate:true}\n",
    "! gsutil -m cp -R gs://vertex-model-garden-public-us/llama3.1/{base_model_name} {MODEL_BUCKET}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_model_vllm(\n",
    "    model_name: str,\n",
    "    model_id: str,\n",
    "    service_account: str,\n",
    "    machine_type: str = \"g2-standard-8\",\n",
    "    accelerator_type: str = \"NVIDIA_L4\",\n",
    "    accelerator_count: int = 1,\n",
    "    gpu_memory_utilization: float = 0.9,\n",
    "    max_model_len: int = 8192,\n",
    "    max_loras: int = 1,\n",
    "    max_cpu_loras: int = 16,\n",
    ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
    "    \"\"\"Deploys trained models with vLLM into Vertex AI.\"\"\"\n",
    "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
    "\n",
    "    vllm_args = [\n",
    "        \"python\",\n",
    "        \"-m\",\n",
    "        \"vllm.entrypoints.api_server\",\n",
    "        \"--host=0.0.0.0\",\n",
    "        \"--port=7080\",\n",
    "        f\"--model={model_id}\",\n",
    "        f\"--tensor-parallel-size={accelerator_count}\",\n",
    "        \"--swap-space=16\",\n",
    "        f\"--gpu-memory-utilization={gpu_memory_utilization}\",\n",
    "        f\"--max-model-len={max_model_len}\",\n",
    "        f\"--max-loras={max_loras}\",\n",
    "        f\"--max-cpu-loras={max_cpu_loras}\",\n",
    "        \"--disable-log-stats\",\n",
    "    ]\n",
    "\n",
    "    env_vars = {\n",
    "        \"MODEL_ID\": model_id,\n",
    "        \"DEPLOY_SOURCE\": \"notebook\",\n",
    "    }\n",
    "\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=model_name,\n",
    "        serving_container_image_uri=VLLM_DOCKER_URI,\n",
    "        serving_container_args=vllm_args,\n",
    "        serving_container_ports=[7080],\n",
    "        serving_container_predict_route=\"/generate\",\n",
    "        serving_container_health_route=\"/ping\",\n",
    "        serving_container_environment_variables=env_vars,\n",
    "        serving_container_shared_memory_size_mb=(16 * 1024),  # 16 GB\n",
    "        serving_container_deployment_timeout=7200,\n",
    "    )\n",
    "    print(\n",
    "        f\"Deploying {model_name} on {machine_type} with {accelerator_count} {accelerator_type} GPU(s).\"\n",
    "    )\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        deploy_request_timeout=1800,\n",
    "        service_account=service_account,\n",
    "    )\n",
    "    print(\"endpoint_name:\", endpoint.name)\n",
    "\n",
    "    return model, endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cb56d402e84a"
   },
   "source": [
    "## Finetune with HuggingFace PEFT and deploy with vLLM on GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set dataset\n",
    "\n",
    "Use the Vertex AI SDK to create and run the custom training jobs.\n",
    "\n",
    "This notebook uses [timdettmers/openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco) dataset as an example.\n",
    "You can set `dataset_name` to any existing [Hugging Face dataset](https://huggingface.co/datasets) name, and set `instruct_column_in_dataset` to the name of the dataset column containing training data. The [timdettmers/openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco) has only one column `text`, and therefore we set `instruct_column_in_dataset` to `text` in this notebook.\n",
    "\n",
    "### (Optional) Prepare a custom JSONL dataset for finetuning\n",
    "\n",
    "You can prepare a JSONL file where each line is a valid JSON string as your custom training dataset. For example, here is one line from the [timdettmers/openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco) dataset:\n",
    "```\n",
    "{\"text\": \"### Human: Hola### Assistant: \\u00a1Hola! \\u00bfEn qu\\u00e9 puedo ayudarte hoy?\"}\n",
    "```\n",
    "\n",
    "The JSON object has a key `text`, which should match `instruct_column_in_dataset`; The value should be one training data point, i.e. a string. After you prepared your JSONL file, you can either upload it to [Hugging Face datasets](https://huggingface.co/datasets) or [Google Cloud Storage](https://cloud.google.com/storage).\n",
    "\n",
    "- To upload a JSONL dataset to [Hugging Face datasets](https://huggingface.co/datasets), follow the instructions on [Uploading Datasets](https://huggingface.co/docs/hub/en/datasets-adding). Then, set `dataset_name` to the name of your newly created dataset on Hugging Face.\n",
    "\n",
    "- To upload a JSONL dataset to [Google Cloud Storage](https://cloud.google.com/storage), follow the instructions on [Upload objects from a filesystem](https://cloud.google.com/storage/docs/uploading-objects). Then, set `dataset_name` to the `gs://` URI to your JSONL file. For example: `gs://cloud-samples-data/vertex-ai/model-evaluation/peft_train_sample.jsonl`.\n",
    "\n",
    "Optionally update the `instruct_column_in_dataset` field below if your JSON objects use a key other than the default `text`.\n",
    "\n",
    "### (Optional) Format your data with custom JSON template\n",
    "\n",
    "Sometimes, your dataset might have multiple text columns and you want to construct the training data with a template. You can prepare a JSON template in the following format:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"description\": \"Template used by Llama 3.1, accepting text-bison format.\",\n",
    "  \"source\": \"https://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-text-models-supervised#dataset-format\",\n",
    "  \"prompt_input\": \"<|start_header_id|>user<|end_header_id|>\\n\\n{input_text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n{output_text}<|eot_id|>\",\n",
    "  \"instruction_separator\": \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "  \"response_separator\": \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "}\n",
    "```\n",
    "\n",
    "As an example, the template above can be used to format the following training data (this line comes from `gs://cloud-samples-data/vertex-ai/model-evaluation/peft_train_sample.jsonl`):\n",
    "\n",
    "```\n",
    "{\"input_text\":\"TRANSCRIPT: \\nREASON FOR EVALUATION:,\\n\\n LABEL:\",\"output_text\":\"Chiropractic\"}\n",
    "```\n",
    "\n",
    "This example template simply concatenates `input_text` with `output_text` with some special tokens in between.\n",
    "\n",
    "To try such custom dataset, you can make the following changes:\n",
    "1. Set `template` to `llama3-text-bison`\n",
    "1. Set `train_dataset_name` to `gs://cloud-samples-data/vertex-ai/model-evaluation/peft_train_sample.jsonl`\n",
    "1. Set `train_split_name` to `train`\n",
    "1. Set `eval_dataset_name` to `gs://cloud-samples-data/vertex-ai/model-evaluation/peft_eval_sample.jsonl`\n",
    "1. Set `eval_split_name` to `train` (**NOT** `test`)\n",
    "1. Set `instruct_column_in_dataset` as `input_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "cellView": "form",
    "id": "KwAW99YZHTdy"
   },
   "outputs": [],
   "source": [
    "# Template name or gs:// URI to a custom template.\n",
    "template = \"openassistant-guanaco\"  # @param {type:\"string\"}\n",
    "\n",
    "# Hugging Face dataset name or gs:// URI to a custom JSONL dataset.\n",
    "train_dataset_name = \"timdettmers/openassistant-guanaco\"  # @param {type:\"string\"}\n",
    "train_split_name = \"train\"  # @param {type:\"string\"}\n",
    "eval_dataset_name = \"timdettmers/openassistant-guanaco\"  # @param {type:\"string\"}\n",
    "eval_split_name = \"test\"  # @param {type:\"string\"}\n",
    "\n",
    "# Name of the dataset column containing training text input.\n",
    "instruct_column_in_dataset = \"text\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune\n",
    "Use the Vertex AI SDK to create and run the custom training jobs.\n",
    "\n",
    "**Note**:\n",
    "1. We recommend setting `finetuning_precision_mode` to `4bit` because it enables using fewer hardware resources for finetuning.\n",
    "1. We recommend using NVIDIA_L4 for 8B models and NVIDIA_A100_80GB for 70B models.\n",
    "1. If `max_steps>0`, it will precedence over `epochs`. One can set a small `max_steps` value to quickly check the pipeline.\n",
    "1. With the default setting, training takes between 1.5 ~ 2 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "cellView": "form",
    "id": "ivVGS9dHXPOz"
   },
   "outputs": [],
   "source": [
    "TRAIN_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-peft-train:20240724_0936_RC00\"\n",
    "\n",
    "# The Llama 3.1 base model.\n",
    "MODEL_ID = os.path.join(MODEL_BUCKET, base_model_name)\n",
    "# The accelerator to use.\n",
    "accelerator_type = \"NVIDIA_L4\"  # @param [\"NVIDIA_L4\", \"NVIDIA_A100_80GB\"]\n",
    "accelerator_count = 4\n",
    "\n",
    "# Batch size for finetuning.\n",
    "per_device_train_batch_size = 1  # @param{type:\"integer\"}\n",
    "gradient_accumulation_steps = 8  # @param{type:\"integer\"}\n",
    "# Maximum sequence length.\n",
    "max_seq_length = 4096  # @param{type:\"integer\"}\n",
    "# Setting a positive `max_steps` here will override `num_epochs`\n",
    "max_steps = -1  # @param{type:\"integer\"}\n",
    "num_epochs = 1.0  # @param{type:\"number\"}\n",
    "# Precision mode for finetuning.\n",
    "finetuning_precision_mode = \"4bit\"  # @param [\"4bit\", \"8bit\", \"float16\"]\n",
    "# Learning rate.\n",
    "learning_rate = 5e-5  # @param{type:\"number\"}\n",
    "lr_scheduler_type = \"cosine\"  # @param{type:\"string\"}\n",
    "# LoRA parameters.\n",
    "lora_rank = 16  # @param{type:\"integer\"}\n",
    "lora_alpha = 32  # @param{type:\"integer\"}\n",
    "lora_dropout = 0.05  # @param{type:\"number\"}\n",
    "enable_gradient_checkpointing = True\n",
    "attn_implementation = \"flash_attention_2\"\n",
    "optimizer = \"paged_adamw_32bit\"\n",
    "warmup_ratio = \"0.01\"\n",
    "report_to = \"tensorboard\"\n",
    "save_steps = 10\n",
    "logging_steps = save_steps\n",
    "\n",
    "replica_count = 1\n",
    "\n",
    "common_util.check_quota(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=REGION,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    is_for_training=True,\n",
    ")\n",
    "\n",
    "job_name = common_util.get_job_name_with_datetime(\"llama3_1-lora-train\").replace(\n",
    "    \"_\", \"-\"\n",
    ")\n",
    "\n",
    "base_output_dir = os.path.join(STAGING_BUCKET, job_name)\n",
    "# Create a GCS folder to store the LORA adapter.\n",
    "lora_output_dir = os.path.join(base_output_dir, \"adapter\")\n",
    "# Create a GCS folder to store the merged model with the base model and the\n",
    "# finetuned LORA adapter.\n",
    "merged_model_output_dir = os.path.join(base_output_dir, \"merged-model\")\n",
    "\n",
    "eval_args = [\n",
    "    f\"--eval_dataset_path={eval_dataset_name}\",\n",
    "    f\"--eval_column={instruct_column_in_dataset}\",\n",
    "    f\"--eval_template={template}\",\n",
    "    f\"--eval_split={eval_split_name}\",\n",
    "    f\"--eval_steps={save_steps}\",\n",
    "    \"--eval_tasks=builtin_eval\",\n",
    "    \"--eval_metric_name=loss\",\n",
    "]\n",
    "\n",
    "train_job_args = [\n",
    "    \"--config_file=vertex_vision_model_garden_peft/deepspeed_zero2_4gpu.yaml\",\n",
    "    \"--task=instruct-lora\",\n",
    "    \"--completion_only=True\",\n",
    "    f\"--pretrained_model_id={MODEL_ID}\",\n",
    "    f\"--dataset_name={train_dataset_name}\",\n",
    "    f\"--train_split_name={train_split_name}\",\n",
    "    f\"--instruct_column_in_dataset={instruct_column_in_dataset}\",\n",
    "    f\"--output_dir={lora_output_dir}\",\n",
    "    f\"--merge_base_and_lora_output_dir={merged_model_output_dir}\",\n",
    "    f\"--per_device_train_batch_size={per_device_train_batch_size}\",\n",
    "    f\"--gradient_accumulation_steps={gradient_accumulation_steps}\",\n",
    "    f\"--lora_rank={lora_rank}\",\n",
    "    f\"--lora_alpha={lora_alpha}\",\n",
    "    f\"--lora_dropout={lora_dropout}\",\n",
    "    f\"--max_steps={max_steps}\",\n",
    "    f\"--max_seq_length={max_seq_length}\",\n",
    "    f\"--learning_rate={learning_rate}\",\n",
    "    f\"--lr_scheduler_type={lr_scheduler_type}\",\n",
    "    f\"--precision_mode={finetuning_precision_mode}\",\n",
    "    f\"--enable_gradient_checkpointing={enable_gradient_checkpointing}\",\n",
    "    f\"--num_epochs={num_epochs}\",\n",
    "    f\"--attn_implementation={attn_implementation}\",\n",
    "    f\"--optimizer={optimizer}\",\n",
    "    f\"--warmup_ratio={warmup_ratio}\",\n",
    "    f\"--report_to={report_to}\",\n",
    "    f\"--logging_output_dir={base_output_dir}\",\n",
    "    f\"--save_steps={save_steps}\",\n",
    "    f\"--logging_steps={logging_steps}\",\n",
    "    f\"--template={template}\",\n",
    "] + eval_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Tensorboard\n",
      "Create Tensorboard backing LRO: projects/255766800726/locations/asia-southeast1/tensorboards/687784105592487936/operations/5507102449564909568\n",
      "Tensorboard created. Resource name: projects/255766800726/locations/asia-southeast1/tensorboards/687784105592487936\n",
      "To use this Tensorboard in another session:\n",
      "tb = aiplatform.Tensorboard('projects/255766800726/locations/asia-southeast1/tensorboards/687784105592487936')\n",
      "Creating TensorboardExperiment\n",
      "TensorboardExperiment created. Resource name: projects/255766800726/locations/asia-southeast1/tensorboards/687784105592487936/experiments/llama3-1-lora-train-20240810-163537\n",
      "To use this TensorboardExperiment in another session:\n",
      "tb experiment = aiplatform.TensorboardExperiment('projects/255766800726/locations/asia-southeast1/tensorboards/687784105592487936/experiments/llama3-1-lora-train-20240810-163537')\n",
      "Training Output directory:\n",
      "gs://my-project-0004-346516-llama31-asia-southeast1/temporal/llama3-1-lora-train-20240810-163537 \n",
      "View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/asia-southeast1/training/1547718196990050304?project=255766800726\n",
      "View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/asia-southeast1/training/1353359725571342336?project=255766800726\n",
      "View tensorboard:\n",
      "https://asia-southeast1.tensorboard.googleusercontent.com/experiment/projects+255766800726+locations+asia-southeast1+tensorboards+687784105592487936+experiments+1353359725571342336\n",
      "CustomContainerTrainingJob projects/255766800726/locations/asia-southeast1/trainingPipelines/1547718196990050304 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/255766800726/locations/asia-southeast1/trainingPipelines/1547718196990050304 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/255766800726/locations/asia-southeast1/trainingPipelines/1547718196990050304 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/255766800726/locations/asia-southeast1/trainingPipelines/1547718196990050304 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/255766800726/locations/asia-southeast1/trainingPipelines/1547718196990050304 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/255766800726/locations/asia-southeast1/trainingPipelines/1547718196990050304 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/255766800726/locations/asia-southeast1/trainingPipelines/1547718196990050304 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "Error: Training failed with:\n",
      "code: 3\n",
      "message: \"The replica workerpool0-0 exited with a non-zero status of 1. Termination reason: Error. To find out more about why your job exited please check the logs: https://console.cloud.google.com/logs/viewer?project=255766800726&resource=ml_job%2Fjob_id%2F1353359725571342336&advancedFilter=resource.type%3D%22ml_job%22%0Aresource.labels.job_id%3D%221353359725571342336%22\"\n",
      ", NVIDIA_L4 failed, trying NVIDIA_A100_80GB\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Custom Training has already run.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 15\u001b[0m\n\u001b[1;32m     14\u001b[0m     machine_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mg2-standard-48\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 15\u001b[0m     \u001b[43mtrain_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_job_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43menvironment_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWANDB_DISABLED\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreplica_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreplica_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmachine_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmachine_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccelerator_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccelerator_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mboot_disk_size_gb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mservice_account\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSERVICE_ACCOUNT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensorboard\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensorboard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresource_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_output_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_output_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/google/cloud/aiplatform/training_jobs.py:4785\u001b[0m, in \u001b[0;36mCustomContainerTrainingJob.run\u001b[0;34m(self, dataset, annotation_schema_uri, model_display_name, model_labels, model_id, parent_model, is_default_version, model_version_aliases, model_version_description, base_output_dir, service_account, network, bigquery_destination, args, environment_variables, replica_count, machine_type, accelerator_type, accelerator_count, boot_disk_type, boot_disk_size_gb, reduction_server_replica_count, reduction_server_machine_type, reduction_server_container_uri, training_fraction_split, validation_fraction_split, test_fraction_split, training_filter_split, validation_filter_split, test_filter_split, predefined_split_column_name, timestamp_split_column_name, timeout, restart_job_on_worker_restart, enable_web_access, enable_dashboard_access, tensorboard, sync, create_request_timeout, disable_retries, persistent_resource_id, tpu_topology)\u001b[0m\n\u001b[1;32m   4771\u001b[0m worker_pool_specs, managed_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_and_validate_run(\n\u001b[1;32m   4772\u001b[0m     model_display_name\u001b[38;5;241m=\u001b[39mmodel_display_name,\n\u001b[1;32m   4773\u001b[0m     model_labels\u001b[38;5;241m=\u001b[39mmodel_labels,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4782\u001b[0m     tpu_topology\u001b[38;5;241m=\u001b[39mtpu_topology,\n\u001b[1;32m   4783\u001b[0m )\n\u001b[0;32m-> 4785\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mannotation_schema_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mannotation_schema_uri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworker_pool_specs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworker_pool_specs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmanaged_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanaged_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparent_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparent_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_default_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_default_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_version_aliases\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_version_aliases\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_version_description\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_version_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4795\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4796\u001b[0m \u001b[43m    \u001b[49m\u001b[43menvironment_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menvironment_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_output_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_output_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice_account\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_account\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbigquery_destination\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbigquery_destination\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_fraction_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_fraction_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_fraction_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_fraction_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_fraction_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_fraction_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_filter_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_filter_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4805\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_filter_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_filter_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4806\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_filter_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_filter_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4807\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredefined_split_column_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredefined_split_column_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4808\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimestamp_split_column_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestamp_split_column_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4809\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4810\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrestart_job_on_worker_restart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrestart_job_on_worker_restart\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4811\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_web_access\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_web_access\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4812\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_dashboard_access\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_dashboard_access\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4813\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensorboard\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensorboard\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4814\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreduction_server_container_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduction_server_container_uri\u001b[49m\n\u001b[1;32m   4815\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreduction_server_replica_count\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[1;32m   4816\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   4817\u001b[0m \u001b[43m    \u001b[49m\u001b[43msync\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msync\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4818\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4819\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4820\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpersistent_resource_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersistent_resource_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4821\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/google/cloud/aiplatform/base.py:863\u001b[0m, in \u001b[0;36moptional_sync.<locals>.optional_run_in_thread.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    862\u001b[0m         VertexAiResourceNounWithFutureManager\u001b[38;5;241m.\u001b[39mwait(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;66;03m# callbacks to call within the Future (in same Thread)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/google/cloud/aiplatform/training_jobs.py:5518\u001b[0m, in \u001b[0;36mCustomContainerTrainingJob._run\u001b[0;34m(self, dataset, annotation_schema_uri, worker_pool_specs, managed_model, model_id, parent_model, is_default_version, model_version_aliases, model_version_description, args, environment_variables, base_output_dir, service_account, network, bigquery_destination, training_fraction_split, validation_fraction_split, test_fraction_split, training_filter_split, validation_filter_split, test_filter_split, predefined_split_column_name, timestamp_split_column_name, timeout, restart_job_on_worker_restart, enable_web_access, enable_dashboard_access, tensorboard, reduction_server_container_uri, sync, create_request_timeout, block, disable_retries, persistent_resource_id)\u001b[0m\n\u001b[1;32m   5501\u001b[0m (\n\u001b[1;32m   5502\u001b[0m     training_task_inputs,\n\u001b[1;32m   5503\u001b[0m     base_output_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5515\u001b[0m     persistent_resource_id\u001b[38;5;241m=\u001b[39mpersistent_resource_id,\n\u001b[1;32m   5516\u001b[0m )\n\u001b[0;32m-> 5518\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_job\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_task_definition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefinition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcustom_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_task_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_task_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mannotation_schema_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mannotation_schema_uri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_fraction_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_fraction_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_fraction_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_fraction_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_fraction_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_fraction_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_filter_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_filter_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_filter_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_filter_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_filter_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_filter_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredefined_split_column_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredefined_split_column_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimestamp_split_column_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestamp_split_column_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5531\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanaged_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparent_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparent_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5534\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_default_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_default_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_version_aliases\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_version_aliases\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_version_description\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_version_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgcs_destination_uri_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_output_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbigquery_destination\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbigquery_destination\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/google/cloud/aiplatform/training_jobs.py:853\u001b[0m, in \u001b[0;36m_TrainingJob._run_job\u001b[0;34m(self, training_task_definition, training_task_inputs, dataset, training_fraction_split, validation_fraction_split, test_fraction_split, training_filter_split, validation_filter_split, test_filter_split, predefined_split_column_name, timestamp_split_column_name, annotation_schema_uri, model, model_id, parent_model, is_default_version, model_version_aliases, model_version_description, gcs_destination_uri_prefix, bigquery_destination, create_request_timeout, block)\u001b[0m\n\u001b[1;32m    851\u001b[0m _LOGGER\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mView Training:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dashboard_uri())\n\u001b[0;32m--> 853\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/google/cloud/aiplatform/training_jobs.py:940\u001b[0m, in \u001b[0;36m_TrainingJob._get_model\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[0;32m--> 940\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_block_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_failed:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/google/cloud/aiplatform/training_jobs.py:983\u001b[0m, in \u001b[0;36m_TrainingJob._block_until_complete\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    981\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(_JOB_WAIT_TIME)\n\u001b[0;32m--> 983\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_failure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    985\u001b[0m _LOGGER\u001b[38;5;241m.\u001b[39mlog_action_completed_against_resource(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/google/cloud/aiplatform/training_jobs.py:1000\u001b[0m, in \u001b[0;36m_TrainingJob._raise_failure\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    999\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gca_resource\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;241m!=\u001b[39m code_pb2\u001b[38;5;241m.\u001b[39mOK:\n\u001b[0;32m-> 1000\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining failed with:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gca_resource\u001b[38;5;241m.\u001b[39merror)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Training failed with:\ncode: 3\nmessage: \"The replica workerpool0-0 exited with a non-zero status of 1. Termination reason: Error. To find out more about why your job exited please check the logs: https://console.cloud.google.com/logs/viewer?project=255766800726&resource=ml_job%2Fjob_id%2F1353359725571342336&advancedFilter=resource.type%3D%22ml_job%22%0Aresource.labels.job_id%3D%221353359725571342336%22\"\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m     accelerator_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNVIDIA_A100_80GB\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     30\u001b[0m     machine_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma2-ultragpu-4g\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 31\u001b[0m     \u001b[43mtrain_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_job_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43menvironment_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWANDB_DISABLED\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreplica_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreplica_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmachine_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmachine_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccelerator_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccelerator_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mboot_disk_size_gb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mservice_account\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSERVICE_ACCOUNT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensorboard\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensorboard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresource_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_output_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_output_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoRA adapter was saved in: \u001b[39m\u001b[38;5;124m\"\u001b[39m, lora_output_dir)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrained and merged models were saved in: \u001b[39m\u001b[38;5;124m\"\u001b[39m, merged_model_output_dir)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/google/cloud/aiplatform/training_jobs.py:4771\u001b[0m, in \u001b[0;36mCustomContainerTrainingJob.run\u001b[0;34m(self, dataset, annotation_schema_uri, model_display_name, model_labels, model_id, parent_model, is_default_version, model_version_aliases, model_version_description, base_output_dir, service_account, network, bigquery_destination, args, environment_variables, replica_count, machine_type, accelerator_type, accelerator_count, boot_disk_type, boot_disk_size_gb, reduction_server_replica_count, reduction_server_machine_type, reduction_server_container_uri, training_fraction_split, validation_fraction_split, test_fraction_split, training_filter_split, validation_filter_split, test_filter_split, predefined_split_column_name, timestamp_split_column_name, timeout, restart_job_on_worker_restart, enable_web_access, enable_dashboard_access, tensorboard, sync, create_request_timeout, disable_retries, persistent_resource_id, tpu_topology)\u001b[0m\n\u001b[1;32m   4768\u001b[0m network \u001b[38;5;241m=\u001b[39m network \u001b[38;5;129;01mor\u001b[39;00m initializer\u001b[38;5;241m.\u001b[39mglobal_config\u001b[38;5;241m.\u001b[39mnetwork\n\u001b[1;32m   4769\u001b[0m service_account \u001b[38;5;241m=\u001b[39m service_account \u001b[38;5;129;01mor\u001b[39;00m initializer\u001b[38;5;241m.\u001b[39mglobal_config\u001b[38;5;241m.\u001b[39mservice_account\n\u001b[0;32m-> 4771\u001b[0m worker_pool_specs, managed_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_and_validate_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4772\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_display_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_display_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4773\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4774\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreplica_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreplica_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4775\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmachine_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmachine_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4776\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccelerator_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4777\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccelerator_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4778\u001b[0m \u001b[43m    \u001b[49m\u001b[43mboot_disk_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mboot_disk_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4779\u001b[0m \u001b[43m    \u001b[49m\u001b[43mboot_disk_size_gb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mboot_disk_size_gb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4780\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreduction_server_replica_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduction_server_replica_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreduction_server_machine_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduction_server_machine_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtpu_topology\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtpu_topology\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4783\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4785\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(\n\u001b[1;32m   4786\u001b[0m     dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[1;32m   4787\u001b[0m     annotation_schema_uri\u001b[38;5;241m=\u001b[39mannotation_schema_uri,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4820\u001b[0m     persistent_resource_id\u001b[38;5;241m=\u001b[39mpersistent_resource_id,\n\u001b[1;32m   4821\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/google/cloud/aiplatform/training_jobs.py:1466\u001b[0m, in \u001b[0;36m_CustomTrainingJob._prepare_and_validate_run\u001b[0;34m(self, model_display_name, model_labels, replica_count, machine_type, accelerator_type, accelerator_count, boot_disk_type, boot_disk_size_gb, reduction_server_replica_count, reduction_server_machine_type, tpu_topology)\u001b[0m\n\u001b[1;32m   1463\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCustom Training is already scheduled to run.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_run:\n\u001b[0;32m-> 1466\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCustom Training has already run.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;66;03m# if args needed for model is incomplete\u001b[39;00m\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_display_name \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_managed_model\u001b[38;5;241m.\u001b[39mcontainer_spec\u001b[38;5;241m.\u001b[39mimage_uri:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Custom Training has already run."
     ]
    }
   ],
   "source": [
    "tensorboard = aiplatform.Tensorboard.create(job_name)\n",
    "exp = aiplatform.TensorboardExperiment.create(\n",
    "    tensorboard_experiment_id=job_name, tensorboard_name=tensorboard.name\n",
    ")\n",
    "\n",
    "# Pass training arguments and launch job.\n",
    "train_job = aiplatform.CustomContainerTrainingJob(\n",
    "    display_name=job_name,\n",
    "    container_uri=TRAIN_DOCKER_URI,\n",
    ")\n",
    "# Worker pool spec.\n",
    "try:\n",
    "    accelerator_type = \"NVIDIA_L4\"\n",
    "    machine_type = \"g2-standard-48\"\n",
    "    train_job.run(\n",
    "        args=train_job_args,\n",
    "        environment_variables={\"WANDB_DISABLED\": True},\n",
    "        replica_count=replica_count,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        boot_disk_size_gb=500,\n",
    "        service_account=SERVICE_ACCOUNT,\n",
    "        tensorboard=tensorboard.resource_name,\n",
    "        base_output_dir=base_output_dir,\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}, NVIDIA_L4 failed, trying NVIDIA_A100_80GB / g2-standard-96\t\")\n",
    "    accelerator_type = \"NVIDIA_L4\" #\"NVIDIA_A100_80GB\"\n",
    "    machine_type = \"g2-standard-96\" #\"a2-ultragpu-4g\"\n",
    "    train_job.run(\n",
    "        args=train_job_args,\n",
    "        environment_variables={\"WANDB_DISABLED\": True},\n",
    "        replica_count=replica_count,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        boot_disk_size_gb=500,\n",
    "        service_account=SERVICE_ACCOUNT,\n",
    "        tensorboard=tensorboard.resource_name,\n",
    "        base_output_dir=base_output_dir,\n",
    "    )\n",
    "\n",
    "print(\"LoRA adapter was saved in: \", lora_output_dir)\n",
    "print(\"Trained and merged models were saved in: \", merged_model_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy\n",
    "This section uploads the model to Model Registry and deploys it on the Endpoint. It takes 15 minutes to 1 hour to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "qmHW6m8xG_4U"
   },
   "outputs": [],
   "source": [
    "print(\"Deploying models in: \", merged_model_output_dir)\n",
    "\n",
    "# Find Vertex AI prediction supported accelerators and regions in [here](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute).\n",
    "if \"8b\" in MODEL_ID.lower():\n",
    "    machine_type = \"g2-standard-8\"\n",
    "    accelerator_type = \"NVIDIA_L4\"\n",
    "    accelerator_count = 1\n",
    "else:\n",
    "    machine_type = \"g2-standard-96\"\n",
    "    accelerator_type = \"NVIDIA_L4\"\n",
    "    accelerator_count = 8\n",
    "\n",
    "common_util.check_quota(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=REGION,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    is_for_training=False,\n",
    ")\n",
    "\n",
    "gpu_memory_utilization = 0.85\n",
    "max_model_len = 8192  # Maximum context length.\n",
    "\n",
    "# Ensure max_model_len does not exceed the limit\n",
    "if max_model_len > 8192:\n",
    "    raise ValueError(\"max_model_len cannot exceed 8192\")\n",
    "\n",
    "models[\"vllm_gpu\"], endpoints[\"vllm_gpu\"] = deploy_model_vllm(\n",
    "    model_name=common_util.get_job_name_with_datetime(prefix=\"llama3_1-vllm-serve\"),\n",
    "    model_id=merged_model_output_dir,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    gpu_memory_utilization=gpu_memory_utilization,\n",
    "    max_model_len=max_model_len,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "\n",
    "Once deployment succeeds, you can send requests to the endpoint with text prompts. Sampling parameters supported by vLLM can be found [here](https://docs.vllm.ai/en/latest/dev/sampling_params.html).\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "Human: What is a car?\n",
    "Assistant:  A car, or a motor car, is a road-connected human-transportation system used to move people or goods from one place to another. The term also encompasses a wide range of vehicles, including motorboats, trains, and aircrafts. Cars typically have four wheels, a cabin for passengers, and an engine or motor. They have been around since the early 19th century and are now one of the most popular forms of transportation, used for daily commuting, shopping, and other purposes.\n",
    "```\n",
    "Additionally, you can moderate the generated text with Vertex AI. See [Moderate text documentation](https://cloud.google.com/natural-language/docs/moderating-text) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads an existing endpoint instance using the endpoint name:\n",
    "- Using `endpoint_name = endpoint.name` allows us to get the\n",
    "  endpoint name of the endpoint `endpoint` created in the cell\n",
    "  above.\n",
    "- Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load\n",
    "  an existing endpoint with the ID 1234567890123456789.\n",
    "You may uncomment the code below to load an existing endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# endpoint_name = \"\"  # @param {type:\"string\"}\n",
    "# aip_endpoint_name = (\n",
    "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
    "# )\n",
    "# endpoint = aiplatform.Endpoint(aip_endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is a car?\"  # @param {type: \"string\"}\n",
    "max_tokens = 50  # @param {type:\"integer\"}\n",
    "temperature = 1.0  # @param {type:\"number\"}\n",
    "top_p = 1.0  # @param {type:\"number\"}\n",
    "top_k = 1  # @param {type:\"integer\"}\n",
    "raw_response = False  # @param {type:\"boolean\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overrides parameters for inferences.\n",
    "If you encounter the issue like `ServiceUnavailable: 503 Took too long to respond when processing`,\n",
    "you can reduce the maximum number of output tokens, such as set max_tokens as 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "2UYUNn60G_4U"
   },
   "outputs": [],
   "source": [
    "instances = [\n",
    "    {\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"top_k\": top_k,\n",
    "        \"raw_response\": raw_response,\n",
    "    },\n",
    "]\n",
    "response = endpoints[\"vllm_gpu\"].predict(instances=instances)\n",
    "\n",
    "for prediction in response.predictions:\n",
    "    print(prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "af21a3cff1e0"
   },
   "source": [
    "## Clean up resources\n",
    "### Delete the model and endpoint\n",
    "Delete the experiment models and endpoints to recycle the resources and avoid unnecessary continouous charges that may incur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_job.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Undeploy model and delete endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for endpoint in endpoints.values():\n",
    "#     endpoint.delete(force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "911406c1561e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for model in models.values():\n",
    "#     model.delete()\n",
    "\n",
    "# delete_bucket = False  # @param {type:\"boolean\"}\n",
    "# if delete_bucket:\n",
    "#     ! gsutil -m rm -r $BUCKET_NAME"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "model_garden_pytorch_llama3_1_finetuning.ipynb",
   "toc_visible": false
  },
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
